{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as t\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A \"network\" for multi-class classification\n",
    "\n",
    "We saw in the previous lab that the two main problems with the perceptron are that it will converge only if the classes are linearly separable and that it can only be used for binary classification.\n",
    "\n",
    "In this section we are going to create a \"network\" where the inputs are directly connected to the output layer, in which we have one output unit for each class.\n",
    "\n",
    "Let's define a *linear layer* as:\n",
    "\n",
    "$$ \\mathbf{z} = W \\mathbf{x} + b $$\n",
    "\n",
    "With $W \\in \\mathbb{R}^{C \\times M} $. $M$ is the dimension of the input and $C$ the number of output units (= number of classes), $\\mathbf{x} \\in \\mathbb{R}^M$ and $b\\in \\mathbb{R}^C$.\n",
    "\n",
    "The Softmax function maps a vector $z \\in \\mathbb{R}^C$ to a vector $q \\in \\mathbb{R}^C$  such that:\n",
    "\n",
    "$$ q_i(\\mathbf{z}) = \\frac{e^{z_i}}{\\sum_{j \\in {\\{1, \\ldots, C\\}}}{e^{z_j}}} \\forall i \\in {\\{1, \\ldots, C\\}} = Softmax(\\mathbf{z})_i $$\n",
    "\n",
    "Note that: $ 0 \\leq q_i \\leq 1 \\forall i \\in {\\{1, \\ldots, C\\}} $, and: $\\sum_{i \\in {\\{1, \\ldots, C\\}}}{q_i} = 1 $; therefore we can interpret the Softmax function as a function that can normalize any real vector $\\mathbf{z}$  into a probability distribution $\\mathbf{q}$ over the $C$ values.\n",
    "\n",
    "With this setup, a \"forward pass\" corresponds to calculating $\\mathbf{z}$ and applying the Softmax to it.\n",
    "\n",
    "The implementation of the Softmax and a forward pass is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Takes the output of a layer as input and returns a probability distribution\n",
    "    input:\n",
    "        z (np.array)\n",
    "\n",
    "    returns:\n",
    "        out (np.array)\n",
    "    \"\"\"\n",
    "    # z_stable = z - np.max(z, axis=0, keepdims=True)  # subtract max per column (use this if you have inf errors)\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "def forward(W: np.ndarray, b: np.ndarray, x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the forward pass and returns the softmax\n",
    "    input:\n",
    "        W (np.array): (N_CLASSES, INPUT_SHAPE) The weight matrix of the perceptron\n",
    "        b (np.array): (N_CLASSES, 1) The bias matrix of the perceptron\n",
    "        x (np.array): (INPUT_SHAPE, 1) The input of the perceptron\n",
    "\n",
    "    returns:\n",
    "        (np.array) (C, 1)\n",
    "    \"\"\"\n",
    "    z = W @ x + b\n",
    "\n",
    "    return softmax(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a supervised classification task, we typically use the cross-entropy function on top of the softmax output as a loss function. We use a 1-hot encoded vector for the true distribution $\\mathbf{p}(x)$ , where we have $1$ in the position corresponding to the true label $y$ and $0$ elsewhere:\n",
    "\n",
    "$$\n",
    "p_i(x) = \\left\\{\n",
    "  \\begin{array}{ll}\n",
    "    1, & \\text{if}\\ y=i \\\\\n",
    "    0, & \\text{otherwise}\n",
    "  \\end{array}\\right.\n",
    "$$\n",
    "\n",
    "and the output of the softmax function as our $\\mathbf{q}$.\n",
    "\n",
    "For a single sample $x$, the cross-entropy loss value for these p(x) and q(x) is then:\n",
    "$$H(p,q)= âˆ’ \\sum_{i=1}^C{p_i(x)\\log q_i(x)}$$\n",
    "\n",
    "Given that the only non-zero element of the 1-hot vector $p(x)$ is at the $y$ index of the correct class, in practice the $p(x)$ vector is a selector for the $y$ index in the $q(x)$ vector. Therefore, the loss function for a single sample becomes:\n",
    "\n",
    "$$ Loss = -\\log(q_y) = - \\log \\left( \\frac{e^{z_y}}{\\sum_{j \\in {\\{1, \\ldots, C\\}}}{e^{z_j}}} \\right) = - z_y + \\log \\sum_{j}{e^{z_j}}$$\n",
    "\n",
    "> **Ques 1**: Derive the gradients of loss with respect to $W$ and $b$.\n",
    "\n",
    "$$\\nabla_W loss = ?$$\n",
    "$$\\nabla_b loss = ?$$\n",
    "\n",
    "\n",
    "\n",
    "> **Task 1**: Implement the gradients and the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grads(\n",
    "    softmaxed: np.ndarray, x: np.ndarray, y: np.ndarray\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "        softmaxed (np.array): (N_CLASSES, batch_size) result of the forward pass\n",
    "        y (np.array): (N_CLASSES, batch_size) One-hot encoded vector of the label\n",
    "        x (np.array): (INPUT_SHAPE, batch_size) Input vector\n",
    "\n",
    "    returns:\n",
    "        d_W (np.array): (N_CLASSES, INPUT_SHAPE) Gradient of the loss with respect to the weight matrix\n",
    "        d_b (np.array): (N_CLASSES, batch_size) Gradient of the loss with respect to the bias matrix\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def compute_loss(softmaxed: np.ndarray, y: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    loss for a single datapoint\n",
    "    inputs:\n",
    "        softmaxed (np.array): (N_CLASSES, batch_size)\n",
    "        y (np.array): (N_CLASSES, batch_size)\n",
    "\n",
    "    returns:\n",
    "        loss value (float) - averaged over a batch\n",
    "    \"\"\"\n",
    "    \n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's generate some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = make_blobs(n_samples=500, n_features=2, random_state=42)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y)\n",
    "plt.show()\n",
    "\n",
    "# encode output as one-hot\n",
    "Y = OneHotEncoder(categories=\"auto\").fit_transform(Y.reshape(-1, 1)).toarray()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, random_state=42, test_size=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function trains our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(\n",
    "    X_train, Y_train, X_test, Y_test, lr, n_epochs=10, batch_size=10, random_seed=42\n",
    "):\n",
    "\n",
    "    INPUT_SHAPE = X_train.shape[1]\n",
    "    N_CLASSES = Y_train.shape[1]\n",
    "\n",
    "    # Initialise metrics lists\n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "    test_acc_history = []\n",
    "\n",
    "    # Initialisation of the weights\n",
    "    np.random.seed(random_seed)\n",
    "    W = np.random.normal(size=(N_CLASSES, INPUT_SHAPE))\n",
    "    b = np.random.normal(size=(N_CLASSES, 1))\n",
    "\n",
    "    n_samples = X_train.shape[0]\n",
    "    n_batches = int(np.ceil(n_samples / batch_size))\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Shuffle dataset\n",
    "        idx = np.random.permutation(n_samples)\n",
    "        X_train = X_train[idx]\n",
    "        Y_train = Y_train[idx]\n",
    "\n",
    "        epoch_loss = []\n",
    "        epoch_acc = []\n",
    "\n",
    "        for b in range(n_batches):\n",
    "            # Mini-batch\n",
    "            start = b * batch_size\n",
    "            end = min((b + 1) * batch_size, n_samples)\n",
    "\n",
    "            X_batch = X_train[start:end].T  # (INPUT_SHAPE, batch_size)\n",
    "            Y_batch = Y_train[start:end].T  # (N_CLASSES, batch_size)\n",
    "\n",
    "            # Forward pass\n",
    "            out = forward(W, b, X_batch)\n",
    "            softmaxed = softmax(out)\n",
    "\n",
    "            # Weight update\n",
    "            d_W, d_b = compute_grads(softmaxed, X_batch, Y_batch)\n",
    "            W -= lr * d_W\n",
    "            b -= lr * d_b\n",
    "\n",
    "            # Batch metrics\n",
    "            epoch_loss.append(compute_loss(softmaxed, Y_batch))\n",
    "            epoch_acc.append(\n",
    "                np.mean(np.argmax(softmaxed, axis=0) == np.argmax(Y_batch, axis=0))\n",
    "            )\n",
    "\n",
    "        # --- End of epoch metrics ---\n",
    "        avg_loss = np.mean(epoch_loss)\n",
    "        avg_acc = np.mean(epoch_acc)\n",
    "        loss_history.append(avg_loss)\n",
    "        acc_history.append(avg_acc)\n",
    "\n",
    "        # Test accuracy\n",
    "        out_test = forward(W, b, X_test.T)\n",
    "        softmaxed_test = softmax(out_test)\n",
    "        avg_test_acc = np.mean(\n",
    "            np.argmax(softmaxed_test, axis=0) == np.argmax(Y_test, axis=1)\n",
    "        )\n",
    "        test_acc_history.append(avg_test_acc)\n",
    "\n",
    "        # print(f\"Epoch {epoch+1}/{n_epochs} - \"\n",
    "        #      f\"Train Loss: {avg_loss:.4f}, \"\n",
    "        #      f\"Train Acc: {avg_acc:.4f}, \"\n",
    "        #      f\"Test Acc: {avg_test_acc:.4f}\")\n",
    "\n",
    "    return W, b, loss_history, acc_history, test_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train the model on the data and observe the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b, loss, acc, test_acc = train_network(\n",
    "    X_train, Y_train, X_test, Y_test, lr=0.01, n_epochs=10\n",
    ")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "ax1.plot(loss)\n",
    "ax1.set_title(\"Training Loss\")\n",
    "ax1.set_xlabel(\"Iterations\")\n",
    "ax2.plot(test_acc, c=\"r\", label=\"test\")\n",
    "ax2.plot(acc, c=\"b\", label=\"train\")\n",
    "ax2.set_title(\"Train and test accuracy\")\n",
    "ax2.set_xlabel(\"Iterations\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we can visualize the decision boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision(X: np.ndarray, Y: np.ndarray, forward_fun: t.Callable, ax=None):\n",
    "    \"\"\"\n",
    "    Plot hard decision regions and scatter data points.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape (N, 2)\n",
    "        Features\n",
    "    Y : np.ndarray, shape (N, C)\n",
    "        One-hot encoded labels\n",
    "    forward_fun : function\n",
    "        Callable that accepts x (shape: (2,1)) and outputs probabilities (C,)\n",
    "    ax : matplotlib Axes, optional\n",
    "        Axis to plot on (useful for subplots). If None, a new figure is created.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Meshgrid over feature space ---\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(\n",
    "        np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200)\n",
    "    )\n",
    "\n",
    "    # --- Forward pass for all grid points ---\n",
    "    grid_points = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "    probs = np.array([forward_fun(pt.reshape(-1, 1)).ravel() for pt in grid_points])\n",
    "\n",
    "    # Hard class assignment\n",
    "    Z = np.argmax(probs, axis=1).reshape(xx1.shape)\n",
    "\n",
    "    # --- Create axis if none provided ---\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Decision surface\n",
    "    ax.pcolormesh(xx1, xx2, Z, cmap=\"viridis\", shading=\"auto\", alpha=0.6)\n",
    "\n",
    "    # Data points\n",
    "    markers = [\".\", \"*\", \"D\", \"o\", \"s\", \"x\"]\n",
    "    for c in range(Y.shape[1]):\n",
    "        ax.scatter(\n",
    "            X[np.argmax(Y, axis=1) == c, 0],\n",
    "            X[np.argmax(Y, axis=1) == c, 1],\n",
    "            c=\"k\",\n",
    "            marker=markers[c % len(markers)],\n",
    "            label=f\"class {c}\",\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"$x_1$\")\n",
    "    ax.set_ylabel(\"$x_2$\")\n",
    "    ax.legend()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision(X_train, Y_train, lambda x: forward(W, b, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we used was linearly separable, so let's take a look at how our network would perform on a different dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = make_moons(n_samples=500, noise=0.2)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y)\n",
    "plt.show()\n",
    "Y = OneHotEncoder(categories=\"auto\").fit_transform(Y.reshape(-1, 1)).toarray()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, random_state=42, test_size=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b, loss, acc, test_acc = train_network(\n",
    "    X_train, Y_train, X_test, Y_test, 0.01, n_epochs=20\n",
    ")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "ax1.plot(loss)\n",
    "ax1.set_title(\"Training Loss\")\n",
    "ax1.set_xlabel(\"Iterations\")\n",
    "ax2.plot(test_acc, c=\"r\", label=\"test\")\n",
    "ax2.plot(acc, c=\"b\", label=\"train\")\n",
    "ax2.set_title(\"Train and test accuracy\")\n",
    "ax2.set_xlabel(\"Iterations\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plot_decision(X_test, Y_test, lambda x: forward(W, b, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A \"real\" neural network\n",
    "\n",
    "Actually our \"network\" until now was nothing else than multinomial Logistic Regression. Therefore, to obtain a non-linear decision boundary, we need at least one **hidden layer** with a non-linear activation function; in this case we are going to study a case in which the activation chosen for the hidden layer is the $ReLU$ function:\n",
    "\n",
    "$$ z = W x + b_1 $$\n",
    "\n",
    "$$ h = ReLU(W x + b_1) = ReLU(z) $$\n",
    "\n",
    "We will use the Softmax seen in the previous case as the output layer:\n",
    "\n",
    "$$ \\theta = U h + b_2 $$\n",
    "\n",
    "$$ \\hat{y} = Softmax(\\theta) $$\n",
    "\n",
    "With $U \\in \\mathbb{R}^{C \\times H}$ and $W \\in \\mathbb{R}^{H \\times M}$ and $b_1$, $b_2$ vectors of corresponding dimensions. $H$ is the number of hidden units.\n",
    "\n",
    "The $ReLU$ function is defined as:\n",
    "\n",
    "$$ ReLU(x) = \n",
    "\\begin{cases}\n",
    "0 \\textrm{ if } x < 0\\\\\n",
    "x \\textrm{ otherwise }\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and it's immediate to observe that the derivative of the ReLU function is: $sgn(ReLU(x)) $.\n",
    "\n",
    "> **Task 2**: Implement the ReLU activation function, the derivative of the relu, and the forward pass for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    input:\n",
    "        x (np.array)\n",
    "\n",
    "    returns:\n",
    "        x (np.array)\n",
    "    \"\"\"\n",
    "    return np.clip(x, 0, np.inf)\n",
    "\n",
    "\n",
    "def d_relu(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Computes the derivative of the relu\n",
    "    input:\n",
    "        x (np.array)\n",
    "\n",
    "    returns:\n",
    "        x (np.array)\n",
    "    \"\"\"\n",
    "    return (x > 0).astype(int)\n",
    "\n",
    "\n",
    "def forward_NN(\n",
    "    U: np.ndarray, b2: np.ndarray, W: np.ndarray, b1: np.ndarray, x: np.ndarray\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Forward pass of a two layer perceptron with relu activation\n",
    "    input:\n",
    "        W (np.array): (HIDDEN_SHAPE, INPUT_SHAPE) The weight matrix of the hidden layer\n",
    "        U (np.array): (N_CLASSES, HIDDEN_SHAPE) The weight matrix of the output layer\n",
    "        b1 (np.array): (HIDDEN_SHAPE, 1) The bias matrix of the hidden layer\n",
    "        b2 (np.array): (N_CLASSES, 1) The bias matrix of the output layer\n",
    "        x (np.array): (INPUT_SHAPE, 1) The input of the perceptron\n",
    "\n",
    "    returns:\n",
    "        softmaxed (np.array): (N_CLASSES, 1) the output of the network after final activation\n",
    "        hidden (np.array): (HIDDENT_SHAPE, 1) the output of the hidden layer after activation\n",
    "        out (np.array): (N_CLASSES, 1) the output of the network before final activation\n",
    "    \"\"\"\n",
    "    hidden = relu(W @ x + b1)\n",
    "    out = U @ hidden + b2\n",
    "\n",
    "    return softmax(out), hidden, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting all of the above together, we can see that:\n",
    "\n",
    "\n",
    "$$ q_i(\\mathbf{\\theta}) = \\frac{e^{\\theta_i}}{\\sum_{j \\in {\\{1, \\ldots, C\\}}}{e^{\\theta_j}}} \\forall i \\in {\\{1, \\ldots, C\\}} = Softmax(\\mathbf{\\theta})_i = \\hat{y}_i$$\n",
    "\n",
    "and therefore the loss for a single datapoint is:\n",
    "\n",
    "$$\n",
    "Loss = -\\log(\\hat{y}_y) = - \\log \\left( \\frac{e^{\\theta_y}}{\\sum_{j \\in {\\{1, \\ldots, C\\}}}{e^{\\theta_j}}} \\right) = - \\theta_y + \\log \\sum_{j}{e^{\\theta_j}}$$\n",
    "\n",
    "> **Ques 2** : Derive the gradients of the loss with respect to $W$, $U$, $b_1$ and $b_2$.\n",
    "\n",
    "\n",
    "\n",
    "> **Task 3**: Based on the previous result implement the computation of the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grads_NN(\n",
    "    hidden: np.ndarray,\n",
    "    softmaxed: np.ndarray,\n",
    "    U: np.ndarray,\n",
    "    X_batch: np.ndarray,\n",
    "    Y_batch: np.ndarray,\n",
    "    W: np.ndarray,\n",
    "    b1: np.ndarray,\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Computes gradients for a two-layer NN with ReLU, batch-aware.\n",
    "\n",
    "    input:\n",
    "        hidden (np.array): (HIDDEN_SHAPE, batch_size) hidden layer activations\n",
    "        softmaxed (np.array): (N_CLASSES, batch_size) output after softmax\n",
    "        U (np.array): (N_CLASSES, HIDDEN_SHAPE) weights of output layer\n",
    "        X_batch (np.array): (INPUT_SHAPE, batch_size) batch of inputs\n",
    "        Y_batch (np.array): (N_CLASSES, batch_size) batch of targets\n",
    "        W (np.array): (HIDDEN_SHAPE, INPUT_SHAPE) weights of hidden layer\n",
    "        b1 (np.array): (HIDDEN_SHAPE, 1) biases of hidden layer\n",
    "\n",
    "    returns:\n",
    "        d_U (np.array): (N_CLASSES, HIDDEN_SHAPE) gradient wrt U\n",
    "        d_b2 (np.array): (N_CLASSES, 1) gradient wrt b2\n",
    "        d_W (np.array): (HIDDEN_SHAPE, INPUT_SHAPE) gradient wrt W\n",
    "        d_b1 (np.array): (HIDDEN_SHAPE, 1) gradient wrt b1\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    return d_U, d_b2, d_W, d_b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function trains the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NN(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    X_test,\n",
    "    Y_test,\n",
    "    lr,\n",
    "    n_hidden,\n",
    "    forward_NN,\n",
    "    compute_grads_NN,\n",
    "    n_epochs=100,\n",
    "    batch_size=10,\n",
    "    random_seed=42,\n",
    "):\n",
    "\n",
    "    HIDDEN_SHAPE = n_hidden\n",
    "    INPUT_SHAPE = X_train.shape[1]\n",
    "    N_CLASSES = Y_train.shape[1]\n",
    "\n",
    "    # Initialise metrics lists\n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "    test_acc_history = []\n",
    "\n",
    "    # Initialisation of the weights\n",
    "    np.random.seed(random_seed)\n",
    "    b1 = np.random.normal(size=(HIDDEN_SHAPE, 1))\n",
    "    W = np.random.normal(size=(HIDDEN_SHAPE, INPUT_SHAPE))\n",
    "    b2 = np.random.normal(size=(N_CLASSES, 1))\n",
    "    U = np.random.normal(size=(N_CLASSES, HIDDEN_SHAPE))\n",
    "\n",
    "    n_samples = X_train.shape[0]\n",
    "    n_batches = int(np.ceil(n_samples / batch_size))\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Shuffle dataset\n",
    "        idx = np.random.permutation(n_samples)\n",
    "        X_train = X_train[idx]\n",
    "        Y_train = Y_train[idx]\n",
    "\n",
    "        epoch_loss = []\n",
    "        epoch_acc = []\n",
    "\n",
    "        for b in range(n_batches):\n",
    "            # Mini-batch\n",
    "            start = b * batch_size\n",
    "            end = min((b + 1) * batch_size, n_samples)\n",
    "            X_batch = X_train[start:end].T  # (INPUT_SHAPE, batch_size)\n",
    "            Y_batch = Y_train[start:end].T  # (N_CLASSES, batch_size)\n",
    "\n",
    "            # Forward pass\n",
    "            softmaxed, hidden, _ = forward_NN(U, b2, W, b1, X_batch)\n",
    "\n",
    "            # Backprop\n",
    "            d_U, d_b2, d_W, d_b1 = compute_grads_NN(\n",
    "                hidden, softmaxed, U, X_batch, Y_batch, W, b1\n",
    "            )\n",
    "\n",
    "            # Update\n",
    "            b1 -= lr * d_b1\n",
    "            W -= lr * d_W\n",
    "            b2 -= lr * d_b2\n",
    "            U -= lr * d_U\n",
    "\n",
    "            # Batch metrics\n",
    "            epoch_loss.append(compute_loss(softmaxed, Y_batch))\n",
    "            epoch_acc.append(\n",
    "                np.mean(np.argmax(softmaxed, axis=0) == np.argmax(Y_batch, axis=0))\n",
    "            )\n",
    "\n",
    "        # --- End of epoch metrics ---\n",
    "        avg_loss = np.mean(epoch_loss)\n",
    "        avg_acc = np.mean(epoch_acc)\n",
    "        loss_history.append(avg_loss)\n",
    "        acc_history.append(avg_acc)\n",
    "\n",
    "        # Test accuracy\n",
    "        softmaxed_test, _, _ = forward_NN(U, b2, W, b1, X_test.T)\n",
    "        avg_test_acc = np.mean(\n",
    "            np.argmax(softmaxed_test, axis=0) == np.argmax(Y_test, axis=1)\n",
    "        )\n",
    "        test_acc_history.append(avg_test_acc)\n",
    "\n",
    "        # print(f\"Epoch {epoch+1}/{n_epochs} - \"\n",
    "        #      f\"Train Loss: {avg_loss:.4f}, \"\n",
    "        #      f\"Train Acc: {avg_acc:.4f}, \"\n",
    "        #      f\"Test Acc: {avg_test_acc:.4f}\")\n",
    "\n",
    "    return U, b2, W, b1, loss_history, acc_history, test_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, b2, W, b1, loss, acc, test_acc = train_NN(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    X_test,\n",
    "    Y_test,\n",
    "    lr=0.1,\n",
    "    n_hidden=16,\n",
    "    forward_NN=forward_NN,\n",
    "    compute_grads_NN=compute_grads_NN,\n",
    ")\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Training loss per epoch\n",
    "ax1.plot(loss, linestyle=\"-\")\n",
    "ax1.set_title(\"Training Loss\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "\n",
    "# Accuracy per epoch\n",
    "ax2.plot(acc, c=\"b\", label=\"train\", linestyle=\"-\")\n",
    "ax2.plot(test_acc, c=\"r\", label=\"test\", linestyle=\"-\")\n",
    "ax2.set_title(\"Train and Test Accuracy\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision(X_test, Y_test, lambda x: forward_NN(U, b2, W, b1, x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify how the network performs if we modify the size of the hidden layer (2, 4, 8, 16 hidden units):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "for i, n_units in enumerate([2, 4, 8, 16]):\n",
    "    U, b2, W, b1, loss, acc, test_acc = train_NN(\n",
    "        X_train,\n",
    "        Y_train,\n",
    "        X_test,\n",
    "        Y_test,\n",
    "        0.1,\n",
    "        n_units,\n",
    "        forward_NN=forward_NN,\n",
    "        compute_grads_NN=compute_grads_NN,\n",
    "    )\n",
    "    plot_decision(\n",
    "        X_test,\n",
    "        Y_test,\n",
    "        lambda x: forward_NN(U, b2, W, b1, x)[0],\n",
    "        plt.subplot(2, 2, i + 1, title=\"%i units\" % n_units),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like now to study the effect of using a different activation function. We will change ReLU to the hyperbolic tangent activation (or *tanh*):\n",
    "\n",
    "$$\n",
    "tanh(x) = \\frac{e^{2x}-1}{e^{2x}+1}\n",
    "$$\n",
    "\n",
    "(Note: you may use the Numpy implementation of tanh)\n",
    "\n",
    "with derivative (necessary to update the gradient computation): $ tanh'(x) =  1 - tanh(x)^2$ \n",
    "\n",
    "\n",
    "> **Task 4**: Update the forward pass and gradient computation of our neural network implementation to use the tanh activation function and vizualize the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_tanh(\n",
    "    U: np.ndarray, b2: np.ndarray, W: np.ndarray, b1: np.ndarray, X_batch: np.ndarray\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Forward pass of a two-layer perceptron with tanh activation\n",
    "    input:\n",
    "        W (HIDDEN_SHAPE, INPUT_SHAPE)\n",
    "        U (N_CLASSES, HIDDEN_SHAPE)\n",
    "        b1 (HIDDEN_SHAPE, 1)\n",
    "        b2 (N_CLASSES, 1)\n",
    "        X_batch (INPUT_SHAPE, batch_size)\n",
    "    returns:\n",
    "        softmaxed (N_CLASSES, batch_size)\n",
    "        hidden (HIDDEN_SHAPE, batch_size)\n",
    "        out (N_CLASSES, batch_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    return softmax(out), hidden, out\n",
    "\n",
    "\n",
    "def compute_grads_tanh(\n",
    "    hidden: np.ndarray,\n",
    "    softmaxed: np.ndarray,\n",
    "    U: np.ndarray,\n",
    "    X_batch: np.ndarray,\n",
    "    Y_batch: np.ndarray,\n",
    "    W: np.ndarray,\n",
    "    b1: np.ndarray,\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Batch-aware gradients for tanh activation\n",
    "\n",
    "    input:\n",
    "        hidden (np.array): (HIDDEN_SHAPE, batch_size) hidden layer activations\n",
    "        softmaxed (np.array): (N_CLASSES, batch_size) output after softmax\n",
    "        U (np.array): (N_CLASSES, HIDDEN_SHAPE) weights of output layer\n",
    "        X_batch (np.array): (INPUT_SHAPE, batch_size) batch of inputs\n",
    "        Y_batch (np.array): (N_CLASSES, batch_size) batch of targets\n",
    "        W (np.array): (HIDDEN_SHAPE, INPUT_SHAPE) weights of hidden layer\n",
    "        b1 (np.array): (HIDDEN_SHAPE, 1) biases of hidden layer\n",
    "\n",
    "    returns:\n",
    "        d_U (np.array): (N_CLASSES, HIDDEN_SHAPE) gradient wrt U\n",
    "        d_b2 (np.array): (N_CLASSES, 1) gradient wrt b2\n",
    "        d_W (np.array): (HIDDEN_SHAPE, INPUT_SHAPE) gradient wrt W\n",
    "        d_b1 (np.array): (HIDDEN_SHAPE, 1) gradient wrt b1\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    return d_U, d_b2, d_W, d_b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation of boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "for i, n_units in enumerate([2, 4, 8, 16]):\n",
    "    U, b2, W, b1, loss, acc, test_acc = train_NN(\n",
    "        X_train,\n",
    "        Y_train,\n",
    "        X_test,\n",
    "        Y_test,\n",
    "        0.1,\n",
    "        n_units,\n",
    "        forward_NN=forward_tanh,\n",
    "        compute_grads_NN=compute_grads_tanh,\n",
    "    )\n",
    "    plot_decision(\n",
    "        X_test,\n",
    "        Y_test,\n",
    "        lambda x: forward_tanh(U, b2, W, b1, x)[0],\n",
    "        plt.subplot(2, 2, i + 1, title=\"%i units\" % n_units),\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
