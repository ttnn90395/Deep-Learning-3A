{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e992d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy.linalg import svd\n",
    "\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4b391d",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks and Deep Learning\n",
    "\n",
    "\n",
    "## The simplest *Neural Network*: the perceptron\n",
    "\n",
    "We can think of the perceptron as the simplest neural network, composed by a single artificial neuron. An artificial neuron is a function of the input $\\mathbf{x}=(x_1, \\ldots , x_M)$ weighted by a vector of connection weights $\\mathbf{w}={w_1, \\ldots w_M}$, completed by a neuron bias $b$ and passed to an **activation function** $\\hat{y}=\\phi(f)$\n",
    "<center>\n",
    "<img src=\"img/perceptron.png\" alt=\"Perceptron\" width=\"350\"/>\n",
    "</center>\n",
    "$$ \\hat{y} = \\phi(\\mathbf{w}^\\top \\mathbf{x} + b )$$\n",
    "\n",
    "Learning with the perceptron consists in updating the weights:\n",
    "\n",
    "$$\\mathbf{w}^{(t+1)}= \\mathbf{w}^t + \\eta (y_i -\\hat{y_i})x_i $$ and\n",
    "$$b^{(t+1)}= b^t + \\eta (y_i -\\hat{y_i})$$\n",
    "\n",
    "with $i \\in {1, \\ldots, N}$, where $\\eta$ is the learning rate, $y_i$ is the correct output for the input $x_i$ and $N$ is the size of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecfe5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "fname='data1.txt'\n",
    "data = np.loadtxt('data/%s' % fname, delimiter=',')\n",
    " \n",
    "X = data[:, 0:2] \n",
    "y = data[:, 2]\n",
    " \n",
    "# Plot data \n",
    "plt.plot(X[:,0][y == 1], X[:,1][y == 1], 'r+')\n",
    "plt.plot(X[:,0][y == 0], X[:,1][y == 0], 'bx')\n",
    "plt.title(fname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c5180c",
   "metadata": {},
   "source": [
    "> **Task 1**: Implement the perceptron algorithm, using the step activation function:\n",
    "$$\n",
    "f(x) =\n",
    "\\begin{cases}\n",
    "0, & x < 0 \\\\\n",
    "1, & x \\geq 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eeb3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    # Step activation\n",
    "    if x >= 0 : return 1\n",
    "    else : return 0\n",
    "\n",
    "def perceptron(W,b,x,y,lr):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      W (np.array): (INPUT_SHAPE, 1) vector of weights\n",
    "      b : bias\n",
    "      x (np.array): (INPUT_SHAPE, 1) input vector (point coordinates)\n",
    "      y : output label (1 or 0)\n",
    "      lr : learning rate\n",
    "    Returns:\n",
    "        (err, W, b) : err is 1 if the predicted label is different from y, 0 otherwise\n",
    "        W, b : the updated weights and bias\n",
    "    \"\"\"\n",
    "    #insert your solution here\n",
    "    \n",
    "    \n",
    "    return (err, W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819c6099",
   "metadata": {},
   "source": [
    "The following cell will run the perceptron on the dataset above and display the *decision boundary*:\n",
    "\n",
    "$\\mathbf{w}^\\top \\mathbf{x} + b = 0$\n",
    "\n",
    "Which for our 2D points corresponds to: $w_0 x_1 + w_1 x_2 + b = 0$\n",
    "\n",
    "Therefore, solving for $x_2$: $x_2 = -\\frac{w_0}{w_1} x_1 - \\frac{b}{w_1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01689da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize weights and b\n",
    "b = 0.0\n",
    "W = rand.normal(size=(X.shape[1], 1))\n",
    "\n",
    "#main algorithm\n",
    "errors=1\n",
    "while errors > 0:\n",
    "    # start epoch\n",
    "    errors=0\n",
    "    for i in range(X.shape[0]):\n",
    "        xi = X[i,:].reshape(-1,1)\n",
    "        yi = y[i]\n",
    "        (err, W, b) = perceptron(W,b,xi,yi, 0.1)\n",
    "        errors+=err\n",
    "    #print(errors)\n",
    "    \n",
    "# plot decision boundary\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X[:,0][y == 1], X[:,1][y == 1], 'r+', label=\"$y=1$\")\n",
    "ax.plot(X[:,0][y == 0], X[:,1][y == 0], 'bx', label=\"$y=0$\")\n",
    "\n",
    "# decision boundary: W0 * x_1 + W1 * x_2 + b = 0\n",
    "x_vals = np.linspace(min(X[:,0]), max(X[:,0]), 200)\n",
    "y_vals = -(W[0] * x_vals + b) / W[1]\n",
    "\n",
    "ax.plot(x_vals, y_vals, 'g-', label=\"Decision boundary\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322a85c1",
   "metadata": {},
   "source": [
    "> **Question 1**: Consider the dataset data2.txt shown below. What is going to be the problem with this dataset? Can you propose a solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee14f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "fname='data2.txt'\n",
    "data = np.loadtxt('data/%s' % fname, delimiter=',')\n",
    " \n",
    "X = data[:, 0:2] \n",
    "y = data[:, 2]\n",
    " \n",
    "# Plot data \n",
    "plt.plot(X[:,0][y == 1], X[:,1][y == 1], 'r+')\n",
    "plt.plot(X[:,0][y == 0], X[:,1][y == 0], 'bx')\n",
    "plt.title(fname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b82fdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd25d23",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning Using PyTorch\n",
    "\n",
    "\n",
    "In this part of the lab we will learn to use <a href=\"https://pytorch.org/\">PyTorch</a> to build more complex neural networks. PyTorch is a high-level API for deep learning. It allows users to implement deep learning models very fast and with minimum effort.\n",
    "\n",
    "The fondamental data structure in PyTorch are <a href=\"https://pytorch.org/docs/stable/tensors.html#torch.Tensor\">Tensors</a>. If you are familiar with Numpy arrays, you will find it easy to adapt to work with tensors. You may also take a look at the [tensor_tutorial](extras/tensor_tutorial.ipynb) for some examples.\n",
    "\n",
    "In this part of the lab, we will implement a simple feedforward neural network to perform classification on a synthetic dataset, of two classes. Your first objective is to create this dataset. It will consist of 200 points in the 2-dimensional space $(N = 200, d = 2)$. Each point will belong either to class 0 or to class 1 (100 points per class), drawn from a Gaussian distribution: \n",
    "\n",
    "$$ \\mathbf{x}_i \\sim \\mathcal{N}(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) $$\n",
    "\n",
    "for class $k$. For class 0, we have $\\boldsymbol{\\mu}_0 = [1,1]$ and the covariance matrix is:\n",
    "\n",
    "$$ \\boldsymbol{\\Sigma}_1 = \\begin{bmatrix} 0.5 & 0 \\\\ 0 & 0.5 \\end{bmatrix} $$\n",
    "\n",
    "For class 1, $\\boldsymbol{\\mu}_1 = [-1,-1]$ and $\\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Sigma}_0$. To generate these values make use of the [`randn`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.randn.html) function of NumPy that returns a sample from the \"standard normal\" distribution as follows: \n",
    "\n",
    "```python\n",
    "sd * np.random.randn(...) + mu\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8ed2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N = 200\n",
    "d = 2\n",
    "num_classes = 2\n",
    "\n",
    "N_per_class=N//2\n",
    "\n",
    "X = np.zeros((N, d))\n",
    "y = np.zeros(N, dtype=np.int64)\n",
    "\n",
    "\n",
    "# Mean vectors\n",
    "mu0 = np.array([1, 1])\n",
    "mu1 = np.array([-1, -1])\n",
    "\n",
    "\n",
    "# Standard deviation (since covariance is diagonal with 0.5)\n",
    "sd = np.sqrt(0.5)\n",
    "\n",
    "\n",
    "np.random.seed(101)\n",
    "# Generate class 0 samples\n",
    "X0 = sd * np.random.randn(N_per_class, d) + mu0\n",
    "y0 = np.zeros(N_per_class)\n",
    "\n",
    "\n",
    "# Generate class 1 samples\n",
    "X1 = sd * np.random.randn(N_per_class, d) + mu1\n",
    "y1 = np.ones(N_per_class)\n",
    "\n",
    "\n",
    "# Combine data\n",
    "X = np.vstack((X0, X1))\n",
    "y = np.hstack((y0, y1))\n",
    "\n",
    "\n",
    "# Shuffle dataset\n",
    "idx = np.random.permutation(N)\n",
    "X = X[idx]\n",
    "y = y[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0fd320",
   "metadata": {},
   "source": [
    "After generating the 200 points, we plot them in a 2-dimensional plane using [`scatter`](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.scatter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29a2b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dataset\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], color='blue', label='Class 0')\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], color='red', label='Class 1')\n",
    "plt.legend()\n",
    "plt.title(\"Synthetic Gaussian Dataset (2 classes)\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9dff2e",
   "metadata": {},
   "source": [
    "> **Task 2**. Split the dataset into a training and a test set using the [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function of scikit-learn. Set the proportion of the dataset to be included in the test set to 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996790ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#your code here (set random state to 42 to have same split as instructors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e527ebc6",
   "metadata": {},
   "source": [
    "Now you will use PyTorch to implement a simple feedforward neural network. We next initialize the CPU (or GPU, if you have CUDA available), and move the data on the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7113838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #if you have a GPU with CUDA installed, this may speed up computation\n",
    "\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).unsqueeze(1).float().to(device)\n",
    "y_test = torch.from_numpy(y_test).unsqueeze(1).float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd73b73",
   "metadata": {},
   "source": [
    "In PyTorch, of particular importance is the notion of a model. The model is the data structure upon which the neural network is built. The most common type of model is the Sequential model, which corresponds to a linear stack of layers. \n",
    "\n",
    "PyTorch provides various implementations for a great number of widely used layers. For instance, to apply a linear transformation:\n",
    "\n",
    "`nn.Linear(input_size, output_size, bias=True)`\n",
    "\n",
    "The layers are organized into <a href=\"https://pytorch.org/docs/stable/nn.html#containers\">Containers</a>. A standard and flexible container is the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module\">Module</a> container. However, if we need a simple sequential network and don't need to reference the different layers, we can use the simplified <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential\">Sequential</a> container.\n",
    "\n",
    "> **Task 3.** Implement a sequential model (named `model`) with an input size of 2, a hidden layer of 64 units with [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) activation. The output layer will contain 1 neuron. This neuron should be activated with a sigmoid function, that ensures that the output corresponds to the probability that an instance belongs to class 1 of our problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337557b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaf4d15",
   "metadata": {},
   "source": [
    "You can visualize the parameters of the model here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3616a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "    \n",
    "torch.save(model.state_dict(), \"./model.st\") #saving the uninitialized model for later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435ff36d",
   "metadata": {},
   "source": [
    "Now we choose the *loss* function and the *optimizer* and we start training our model.\n",
    "Take a look at the various <a href=\"https://pytorch.org/docs/stable/optim.html\">optimizers</a> and <a href=\"https://pytorch.org/docs/stable/nn.html#loss-functions\">loss</a> functions available in Torch. In our case,we are going to use the Binary Cross Entropy: \n",
    "\n",
    "$$\\mathcal{L}_{\\text{BCE}} \n",
    "= - \\frac{1}{N} \\sum_{i=1}^N \\Big( \n",
    "y_i \\, \\log p_\\theta(\\mathbf{x}_i) + \n",
    "(1 - y_i) \\, \\log \\big( 1 - p_\\theta(\\mathbf{x}_i) \\big) \n",
    "\\Big)$$\n",
    "\n",
    "where $p_\\theta(\\mathbf{x}_i)$ is the output value of our network after the sigmoid, indicating the predicted probability of $\\mathbf{x}_i$ belonging to class $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaddcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device) #if you have CUDA, this will make computation faster\n",
    "\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9) #actually SGD is just GD in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e581a992",
   "metadata": {},
   "source": [
    "Once compiled, we can train the model for 5 *epochs* (that is, how many times the model \"sees\" the whole training set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ba2e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    outputs = model.forward(X_train)\n",
    "    loss = loss_function(outputs, y_train)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e15ff2",
   "metadata": {},
   "source": [
    "In this case we updated the weights at every pass by looking at the gradient estimated on the full training set. This is commonly known as **gradient descent**:\n",
    "\n",
    "$$\\theta \\leftarrow \\theta - \\eta \\, \\frac{1}{N} \\sum_{i=1}^{N} \\nabla_\\theta L(\\mathbf{x_i}, y_i)$$\n",
    "\n",
    "With large dataset, it is preferable update the weights with smaller random subsets of the training set (the training **batches**). In this case we talk about **stochastic gradient descent**.\n",
    "\n",
    "$$\\theta \\leftarrow \\theta - \\eta \\, \\frac{1}{B} \\sum_{i\\in \\mathcal{B}_t} \\nabla_\\theta L(\\mathbf{x_i}, y_i)$$\n",
    "\n",
    "where $i \\in \\mathcal{B}_t \\subset \\{1,2,\\dots,N\\}, \\quad |\\mathcal{B}_t| = B$\n",
    "\n",
    "the mini-batches $\\{ \\mathbf{x_i} | i \\in \\mathcal{B}_t \\}$ are drawn without replacement within an epoch (the dataset is shuffled once per epoch). This way, each sample is seen exactly once per epoch.\n",
    "\n",
    "In PyTorch, you can create training batches as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52edc708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1a78e4",
   "metadata": {},
   "source": [
    "> **Task 4.** Re-run the training loop using SGD with batches of size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a923b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting the model to pre-training state:\n",
    "model.load_state_dict(torch.load(\"./model.st\", weights_only=True))\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13009a3",
   "metadata": {},
   "source": [
    "> **Question 2**: Can you suggest a reason for which the loss after every epoch is lower with SGD?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a904bb2a",
   "metadata": {},
   "source": [
    "Once trained, we can use your model to generate predictions on new data. Predictions are real values between 0 and 1. Set predictions larger than 0.5 to 1 and predictions smaller than 0.5 to 0.\n",
    "\n",
    "> **Task 5**. Evaluate the result of the model on X_test using `from sklearn.metrics import accuracy_score`. NB: take into account that the output of the model will be a Tensor and you will need to convert to numpy again to use `accuracy_score`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a3f1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a0f369",
   "metadata": {},
   "source": [
    "Now, we are going to evaluate the error accuracy on training and test set during the training process and plot it.\n",
    "\n",
    "> **Task 6**: Complete the following code to store the loss, the training accuracy and test accuracy, per batch, in the `losses` , `tr_acc` and `test_acc` arrays respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5478bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting the model to pre-training state:\n",
    "model.load_state_dict(torch.load(\"./model.st\", weights_only=True))\n",
    "\n",
    "#init performance measures\n",
    "losses = []\n",
    "tr_acc = []\n",
    "test_acc = []\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    for batch_X, batch_y in dataloader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = loss_function(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,8))\n",
    "ax1.plot([loss for loss in losses])\n",
    "ax1.set_title(\"Training Loss\")\n",
    "ax1.set_xlabel(\"Iterations\")\n",
    "ax2.plot(test_acc, c='r', label='test')\n",
    "ax2.plot(tr_acc, c='b', label='train')\n",
    "ax2.set_title(\"Train and test accuracy\")\n",
    "ax2.set_xlabel(\"Iterations\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3899a5c",
   "metadata": {},
   "source": [
    "Using the `pcolormesh` function, we can plot the decision surface of the network with regard to the input space. This is demonstrated as follows. Use your code from above (using `scatter`) to plot the data points over this surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db7c651",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "# Create meshgrid\n",
    "xx1, xx2 = np.meshgrid(\n",
    "    np.linspace(X[:,0].min()-1, X[:,0].max()+1, num=100),\n",
    "    np.linspace(X[:,1].min()-1, X[:,1].max()+1, num=100)\n",
    ")\n",
    "\n",
    "# Convert to tensor for model input\n",
    "grid_points = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "grid_tensor = torch.tensor(grid_points, dtype=torch.float32)\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():  # no gradients needed for plotting\n",
    "    p_y = model(grid_tensor)\n",
    "\n",
    "# Convert back to NumPy for plotting\n",
    "p_y = p_y.cpu().numpy()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,8))\n",
    "\n",
    "# --- Titles & labels for subplots ---\n",
    "ax1.set_title(\"Surface $P(Y | \\\\mathbf{X})$ (Train Data)\")\n",
    "ax1.set_xlabel(\"$x_1$\")\n",
    "ax1.set_ylabel(\"$x_2$\")\n",
    "\n",
    "ax2.set_title(\"Surface $P(Y | \\\\mathbf{X})$ (Test Data)\")\n",
    "ax2.set_xlabel(\"$x_1$\")\n",
    "ax2.set_ylabel(\"$x_2$\")\n",
    "\n",
    "# --- Plot decision surfaces ---\n",
    "pcm = ax1.pcolormesh(xx1, xx2, p_y.reshape(xx1.shape), cmap='RdBu', shading=\"auto\")\n",
    "ax1.pcolormesh(xx1, xx2, p_y.reshape(xx1.shape), cmap='RdBu')\n",
    "ax2.pcolormesh(xx1, xx2, p_y.reshape(xx1.shape), cmap='RdBu')\n",
    "\n",
    "# --- Plot data ---\n",
    "ax1.scatter(X_train[:,0], X_train[:,1], c=y_train, edgecolor=\"k\")\n",
    "ax2.scatter(X_test[:,0], X_test[:,1], c=y_test, edgecolor=\"k\")\n",
    "\n",
    "# Shared colorbar\n",
    "\n",
    "fig.colorbar(pcm, ax=[ax1, ax2], shrink=0.8, label=\"$P(Y | \\\\mathbf{X})$\")\n",
    "\n",
    "#plt.tight_layout()  # avoid overlap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8244583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
